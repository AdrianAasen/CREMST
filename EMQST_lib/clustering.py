import numpy as np
import scipy.cluster.hierarchy as sch
import copy
from itertools import combinations
from EMQST_lib import overlapping_tomography as ot


def fcluster_to_labels(fcluster_labels):
    """
    Converts flat cluster labels into overlapping cluster labels.
    This function takes a list of cluster labels generated by the `fcluster` function
    and converts it into a list of lists, where each sublist contains the indices of 
    the elements that belong to the same cluster. It also removes any empty clusters.
    Parameters:
    fcluster_labels (list of int): A list of cluster labels where each element represents 
                                the cluster assignment of the corresponding data point.
    Returns:
    list of list of int: A list of clusters, where each cluster is represented as a list 
                        of indices of the data points that belong to that cluster.
    """
    cluster = [[] for _ in range(max(fcluster_labels))]
    for i in range(len(fcluster_labels)):
        cluster[fcluster_labels[i]-1].append(i)
        
    # If some groups are empty
    while [] in cluster:
        cluster.remove([])
    return cluster



def split_oversized_clusters(linkage, fcluster_labels, max_cluster_size):
    """
    Given a hierarchical clustering dendrogram (linkage matrix), the flat cluster labels
    (e.g. from fcluster), and a mutual distance vector, perform one additional “cut”
    for each cluster that exceeds max_cluster_size. The cut is made at the first available
    intersection (i.e. at the node’s immediate children in the dendrogram).

    Parameters
    ----------
    linkage : ndarray
        The linkage matrix (as returned by, e.g., scipy.cluster.hierarchy.linkage).
    fcluster_labels : array-like
        1D array of cluster labels (e.g. produced by scipy.cluster.hierarchy.fcluster).
        It is assumed that these clusters correspond to subtrees in the dendrogram.
    mutual_distance : array-like
        A vector of mutual distances (e.g. the distances from the linkage matrix).
        (Not used in this simple one-cut routine but provided for potential extensions.)
    max_cluster_size : int
        The maximum allowed cluster size. Any cluster with more observations than this
        will be split at its first available dendrogram intersection.
    
    Returns
    -------
    new_labels : ndarray
        A copy of fcluster_labels with new labels assigned to the newly split clusters.
        Each oversized cluster is split only once (its node is replaced by its two immediate children).
    """
    # Convert the linkage matrix to a tree structure.
    # 'to_tree' returns the root node and a list of all nodes.
    root, _ = sch.to_tree(linkage, rd=True)
    
    # Make a copy of the labels to update.
    new_labels = np.array(fcluster_labels, copy=True)
    current_max_label = new_labels.max()
    
    def find_node_for_cluster(node, target_set):
        """
        Recursively search for the node whose leaves exactly match target_set.
        target_set is a set of indices (0-indexed) of the original observations.
        """
        # If the current node's leaves match exactly, return it.
        if set(node.pre_order()) == target_set:
            return node
        # If leaf, cannot split further.
        if node.is_leaf():
            return None
        # Otherwise, search in the children.
        left_result = find_node_for_cluster(node.left, target_set)
        if left_result is not None:
            return left_result
        return find_node_for_cluster(node.right, target_set)
    
    # Process each unique cluster label
    for label in np.unique(fcluster_labels):
        # Get the indices belonging to this cluster
        cluster_idx = np.where(fcluster_labels == label)[0]
        if len(cluster_idx) > max_cluster_size:
            target_set = set(cluster_idx)
            node = find_node_for_cluster(root, target_set)
            # Only split if we found a matching node and it is not a leaf.
            if node is not None and not node.is_leaf():
                # Use the immediate children as the split clusters.
                left_leaves = node.left.pre_order()
                right_leaves = node.right.pre_order()
                # In this simple version, we keep one branch with the original label and assign
                # a new label to the other branch.
                # (You could choose based on additional criteria, e.g. using mutual_distance.)
                current_max_label += 1
                # Here, we assign the right branch the new label.
                new_labels[right_leaves] = current_max_label
                # The left branch keeps the original label.
    return new_labels



def hierarchical_clustering(dist_matrix, threshold = None,  method = None):
    """
    Implements Scipy's hierarchical clustering algorithm. Default is complete linkage method.
    Ward's method works well in our tested cases, but the distance metric is not Euclidean and can behave unpredictably, therefore not recommended.
    """
    
    if threshold is None:
        threshold = 1 # If no threadhold is spesified, we select the upper limit.
    if method is None:
        method = 'complete'
    
    np.fill_diagonal(dist_matrix, 0)  # Diagonal should be 0
    condensed_distance_matrix = sch.distance.squareform(dist_matrix)
    Z = sch.linkage(condensed_distance_matrix, method=method)
    cluster_labels = sch.fcluster(Z, t=threshold, criterion='distance')
    return cluster_labels, Z # Returns Z for plotting


def create_distance_matrix_from_corr(corr_array, unique_corr_labels, n_qubits):
    corr_matrix = np.zeros((n_qubits,n_qubits))
    it = 0
    for i,j in unique_corr_labels:
        corr_matrix[i,j] = corr_array[it] 
        corr_matrix[j,i] = corr_array[it] 
        it += 1
    return (1 - corr_matrix)

def are_sublists_equal(list1, list2):
    """
    Checks if every sublist in list1 exists in list2 and vice versa.

    Args:
        list1 (list of lists): The first list of lists.
        list2 (list of lists): The second list of lists.

    Returns:
        bool: True if both lists contain the same sublists, otherwise False.
    """
    # Convert sublists to tuples for comparison (tuples are hashable)
    set1 = {tuple(sublist) for sublist in list1}
    set2 = {tuple(sublist) for sublist in list2}
    
    return set1 == set2



def assign_init_cluster(cluster_correlator_array,corr_labels,n_qubits):
    """
    Creates initial cluster by sorting for highest correlation coefficients, 
    and grouping those qubit pairs together.
    
    """
    partitions = []
    it = 1
    while len(partitions) < n_qubits/2:
        index = np.argpartition(cluster_correlator_array, -(it))[-(it):][0]
        if not np.any(np.isin(corr_labels[index],partitions )):
            partitions.append(corr_labels[index].tolist())
            # print(f'Correlation strenght: {cluster_correlator_array[index]}')
            # print(f'Current partition: {partitions}')
        it+=1
    return partitions



def find_noise_cluster_structure(QDT_outcomes, n_qubits, n_QDT_shots, hash_family, n_hash_symbols, one_qubit_calibration_states, n_cores):
    """
    This is now an defuct function, and should not be used. It is kept for reference only. 
    """
    
    print(f'Create all possible 2 qubit POVMs for correlation map.')
    two_point_POVM, corr_subsystem_labels = ot.reconstruct_all_two_qubit_POVMs(QDT_outcomes, n_qubits, hash_family, n_hash_symbols, one_qubit_calibration_states, n_cores)
    summed_quantum_corr_array, unique_corr_labels = ot.compute_quantum_correlation_coefficients(two_point_POVM, corr_subsystem_labels)
    # Find the full cluster size of the system
    n_runs  = 5
    alpha = 0.4
    cluster_max = 4
    # Create initialpartition
    expected_large_values = n_qubits*((cluster_max-1))
    two_point_init_cluster = assign_init_cluster(summed_quantum_corr_array,unique_corr_labels,n_qubits)
    # Optimize cluster structure according to heuristic cost function. 
    two_noise_cluster_labels_temp, two_reward_temp = optimize_cluster(n_runs,two_point_init_cluster,summed_quantum_corr_array,unique_corr_labels,cluster_max,expected_large_values,alpha)

    return two_noise_cluster_labels_temp


def optimize_cluster(n_runs,init_partition,corr_array,corr_labels,max_cluster_size, expected_large_values, alpha ):
    """
    Cluster optimization loop.
    This is now an defuct function, and should not be used. It is kept for reference only. 
    """
    #print('Starting optimization of premade cluster structure.')
    rng = np.random.default_rng()
    best_partition = copy.deepcopy(init_partition)
    reward = obj_func(best_partition,corr_array,corr_labels,max_cluster_size,expected_large_values,alpha)
    repetitions = 3
    for _ in range(repetitions):
        partition = copy.deepcopy(init_partition)
        for i in range(n_runs):
            #print('Run:',i)
            S_pairs = copy.deepcopy(corr_labels)
            S_pairs = rng.permutation(S_pairs)
            cost_0 = obj_func(partition, corr_array, corr_labels,max_cluster_size, expected_large_values, alpha)
            for pair in S_pairs:
                #print(pair,parition,np.isin(partitions,pair))
                if is_pair_in_more_than_one_cluster(pair,partition): # If pairs exist in more than one cluster:
                    
                    masked_partition = []  # Retrieve the partitions that include the pair
                    new_partition_1 = copy.deepcopy(partition)
                    temp_count = 0
                    for i in range(len(partition)): # Create a partition with the pair removed and one with just the pair
                        if np.any(np.isin(partition[i],pair)):
                            masked_partition.append(partition[i])
                            new_partition_1.pop(i-temp_count)
                            temp_count+=1
                    #print(f'Partion removed:{new_partition_1}')
                    #print(f'Parition added {masked_partition}')
                    #partion_mask = np.any(np.isin(parition,pair),axis=1) # Create mask for where what clusters include any of element in the pair

                    if len(masked_partition)>2:
                        print("Pair is assigned to more than 2 clusters.")
                        print(masked_partition)
                        return 0
                    # We now check 3 instances of these partitions:
                    # 1) Swap 1st qubit to second
                    # 2) Swat 2nd qubit to first
                    # 3) Exchange qubits between the two partitions
                    
                    new_partition_2 = copy.deepcopy(new_partition_1)
                    new_partition_3 = copy.deepcopy(new_partition_1)
                    #print(pair)
                    # 1) Swap 1st qubit to second
                    masked_partition_1 = copy.deepcopy(masked_partition)
                    #print(f'Original: {new_partition_1}, {masked_partition}, {pair}')
                    if pair[0] in masked_partition_1[0]:
                        masked_partition_1[0].remove(pair[0])
                        masked_partition_1[1].append(pair[0])
                    else:
                        masked_partition_1[1].remove(pair[0])
                        masked_partition_1[0].append(pair[0])
                    new_partition_1.append(masked_partition_1[0])
                    new_partition_1.append(masked_partition_1[1])

                    # 2) Swap 2nd qubit to first
                    masked_partition_2 = copy.deepcopy(masked_partition)
                    if pair[1] in masked_partition_2[0]:
                        masked_partition_2[0].remove(pair[1])
                        masked_partition_2[1].append(pair[1])
                    else:
                        masked_partition_2[1].remove(pair[1])
                        masked_partition_2[0].append(pair[1])
                    new_partition_2.append(masked_partition_2[0])
                    new_partition_2.append(masked_partition_2[1])
                
                    # 3) Exchange qubits between the two partitions
                    masked_partition_3 = copy.deepcopy(masked_partition)
                    if pair[0] in masked_partition_3[0]:
                        masked_partition_3[0].remove(pair[0])
                        masked_partition_3[1].append(pair[0])
                        masked_partition_3[1].remove(pair[1])
                        masked_partition_3[0].append(pair[1])
                    else:
                        masked_partition_3[1].remove(pair[0])
                        masked_partition_3[0].append(pair[0])
                        masked_partition_3[0].remove(pair[1])
                        masked_partition_3[1].append(pair[1])
                    new_partition_3.append(masked_partition_3[0])
                    new_partition_3.append(masked_partition_3[1])
                    #print(new_partition_1)
                    for new_partition in [new_partition_1,new_partition_2,new_partition_3]:
                        cost = obj_func(new_partition,corr_array,corr_labels,max_cluster_size,expected_large_values,alpha)
                        #print(cost)
                        if cost > cost_0:
                            #print(f'New partition {new_partition}')
                            #print('Cost:',cost)
                            partition = copy.deepcopy(new_partition)
                            cost_0 = cost
                            
                            
                else: # If pair is in the same cluster, try to remove one from the cluster. 
                    masked_partition = []  # Retrieve the partitions that include the pair
                    new_partition_1 = copy.deepcopy(partition)
                    temp_count = 0
                    for o in range(len(partition)): # Create a partition with the pair removed and one with just the pair
                        if np.any(np.isin(partition[o],pair)):
                            masked_partition.append(partition[o])
                            new_partition_1.pop(o-temp_count)
                            temp_count+=1
                    # We remove one qubit into a new partition, then the other qubit into a new partition.
                    
                    new_partition_2 = copy.deepcopy(new_partition_1)
                    new_partition_3 = copy.deepcopy(new_partition_1)
                    masked_partition_1 = copy.deepcopy(masked_partition)
                    masked_partition_2 = copy.deepcopy(masked_partition)
                    masked_partition_3 = copy.deepcopy(masked_partition)
                    # Put first qubit into a new partition
                    masked_partition_1[0].remove(pair[0])
                    new_partition_1.append(masked_partition_1[0])
                    new_partition_1.append([pair[0]])
                    
                    # Put second qubit into new partition
                    masked_partition_2[0].remove(pair[1])
                    new_partition_2.append(masked_partition_2[0])
                    new_partition_2.append([pair[1]])
                    
                    # Remove both
                    masked_partition_3[0].remove(pair[1])
                    masked_partition_3[0].remove(pair[0])
                    new_partition_3.append(masked_partition_3[0])
                    new_partition_3.append([pair[1]])
                    new_partition_3.append([pair[0]])
                    
                    for new_partition in [new_partition_1,new_partition_2, new_partition_3]:
                        cost = obj_func(new_partition,corr_array,corr_labels,max_cluster_size,expected_large_values,alpha)
                        if cost > cost_0:
                            #print(f'New partition {new_partition}')
                            #print('Cost:',cost)
                            #print('Created a new parition!')
                            partition = copy.deepcopy(new_partition)
                            cost_0 = cost
                
        if obj_func(partition,corr_array,corr_labels,max_cluster_size,expected_large_values,alpha) > reward:
            reward = obj_func(partition,corr_array,corr_labels,max_cluster_size,expected_large_values,alpha)
            best_partition = copy.deepcopy(partition)
    while [] in best_partition: # Remove empty partitions before sending back
        best_partition.remove([])
    best_partition = [np.sort(part)[::-1] for part in best_partition]           
    return best_partition, reward


def obj_func(partitions,corr_array,corr_labels, max_cluster_size, expected_large_values, alpha):
    """
    This is now an defuct function, and should not be used. It is kept for reference only. 
    Objective for the cluster opitmization problem.
    Alpha: A tuning paramter, which tunes the penalty for large clusters. Large alpha discuourages large clusters.
    """
    cost = 0
    # Calculate the current cluster strenght

    S = np.zeros(len(partitions))
    for i in range(len(partitions)):
        mask = np.all(np.isin(corr_labels,partitions[i]),axis=1)
        #print(mask,partitions[i])
        S[i] = np.sum(corr_array[mask])
    S_sum = np.sum(S)
    partition_size = np.array([len(partition) for partition in partitions])
    
    # Finding c_avg
    large_corr = np.sort(np.abs(corr_array))[-expected_large_values:]
    c_avg = np.mean(large_corr)
    #print(f'New avg: {c_avg}')
    for i in range(len(partitions)):
        if partition_size[i] > max_cluster_size:
            cost -= 1e10
        else:
            cost -=c_avg*alpha*partition_size[i]**2
    return cost + S_sum


def is_pair_in_more_than_one_cluster(pair_label, clusters):
    """
    Check if a pair is in more than one cluster.
    Takes in a pair of qubits, and check the list of clusters if the pair is present in more than one cluster.
    """
    counter = 0
    for cluster in clusters:
        if np.any(np.isin(pair_label,cluster)):
            counter +=1
    if counter > 1:
        return True
    else:    
        return False

def get_true_cluster_labels(cluster_size):
    "Takes in cluster size_size and returns the qubit labels for the true states clustered together."
    true_cluster_labels = []
    it = 0
    rev_cluster_size = cluster_size[::-1]
    for i in range(len(cluster_size)):
        true_cluster_labels.append(np.arange(it, it + rev_cluster_size[i]))
        it+=rev_cluster_size[i]
    true_cluster_labels = [np.sort(cluster)[::-1] for cluster in true_cluster_labels]
    return true_cluster_labels[::-1]


def find_clusters_from_correlator_labels(correlator_labels, clusters):
    """
    From the correlation labels finds the clusters that contain any of the labels.

    Parameters:
    - correlator_labels (list): A list of correlation labels.
    - clusters (list): A list of clusters.

    Returns:
    - return_cluster (list): A list of clusters that contain any of the correlator labels.
    """
    return_cluster = []
    for label in correlator_labels:
        temp_cluster = []
        for cluster in clusters:
            if np.any(np.isin(cluster, label)): # label is in cluster
                temp_cluster.append(cluster) # Adds cluster to label
            
        return_cluster.append(temp_cluster)
    return return_cluster


def is_hash_family_perfect(hash_list, k_hash_symbols):
    """
    Checks if a collection of hash functions (hash_list) forms a perfect hash family
    for subsets of size k_hash_symbols.
    """
    n_qubits = hash_list.shape[1]

    # Generate all subsets of size k_hash_symbols
    subset_indices = list(combinations(range(n_qubits), k_hash_symbols))

    # For each subset of size k_hash_symbols, we need at least one hash function
    # that assigns distinct labels to those elements in the subset.
    for subset in subset_indices:
        found_distinguishing_hash = False
        for hash_func in hash_list:
            labels = hash_func[list(subset)]
            # If these k qubits get k distinct labels in 'hash_func':
            if len(np.unique(labels)) == k_hash_symbols:
                found_distinguishing_hash = True
                break
        if not found_distinguishing_hash:
            return False
    return True